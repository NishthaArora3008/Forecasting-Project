---
title: "Retail Forecasting Project Report"
subtitle: "ETC5550-Applied Forecasting-S1, 2023"
author: "Nishtha Arora (32296622)"
Date: "`r Sys.Date()`"
output:
    bookdown::html_document2:
      theme: journal
      toc: true
      toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  out.width = "80%"
)

library(fpp3)
library(tidyverse)
library(kableExtra)
library(viridis)
library(readabs)
library(magick)
```

```{r, echo=FALSE, fig.align='center', fig.width=10}
image <-
  image_read(
    "https://thumbs.dreamstime.com/b/businessman-looking-far-economic-forecasting-concept-businessman-looking-far-economic-forecasting-concept-153410971.jpg"
  )
image

```

**Objective: To forecast a real time series using ETS and ARIMA models.**

```{r dataset}
set.seed(32296622)
myseries <- aus_retail |>
  
  filter(!(
    `Series ID` %in% c(
      "A3349561R",
      "A3349883F",
      "A3349499L",
      "A3349902A",
      "A3349588R",
      "A3349763L",
      "A3349372C",
      "A3349450X",
      "A3349679W",
      "A3349378T",
      "A3349767W",
      "A3349451A"
    )
  )) |>
  filter(`Series ID` == sample(`Series ID`, 1))
```

# Statistical features of the original data.

## Data Description

The data set used is a subset of the *aus_retail* data which contains one numeric variable, i.e. Turnover in $Million AUD. *aus_retail* is a part of *tsibbledata* package and can also be loaded with the package *fpp3* which contains total 9 packages including tsibbledata.

The data is a time series of class *tsibble* and the source for the same is [Australian Bureau of Statistics, catalogue number 8501.0, table 11.](https://www.abs.gov.au/ausstats/abs@.nsf/exnote/8501.0)(Robjhyndman, n.d.).

The sample data set contains the Turnover data for the *'hardware, building and garden supplies retailing'* for the state of *'South Australia'* for the years *'1982 to 2018'* (calculated each month from April 1982 to December 2018) and contains `r nrow(myseries)` observations and `r ncol(myseries)` columns.

The dataset contain the following variables and table \@ref(tab:tab1) displays the variable type as well.

```{r checking_variabletypes, include=FALSE}
str(myseries)

```

```{r tab1}
data_dict <- data.frame(
  S.No. = c("1", "2", "3", "4", "5"),
  Variables = c ("State", "Industry", "Series ID", "Month", "Turnover"),
  DataType = c("Character", "Character", "Character", "Month", "Numeric")
)


knitr::kable (data_dict , caption = "Data Dictonary") |>
  kable_styling(latex_options = c("striped", "hold_position")) |>
  kable_paper("hover", full_width = T) |>
  scroll_box(width = "100%", height = "300px")
```

## Data view

```{r viewing_data}
myseries

```

## Simple Statistic features

### Yearly average of Turnover 

Table \@ref(tab:tab2) displays an yearly average turnover in $Million AUD.

```{r tab2}
year_average <- myseries |>
  mutate(year = year(Month)) |>
  index_by(year) |>
  summarise(Total_turnover = mean(Turnover))

knitr::kable (year_average , caption = "Yearly Turnover Summary") |>
  kable_styling(latex_options = c("striped", "hold_position")) |>
  kable_paper("hover", full_width = T) |>
  scroll_box(width = "100%", height = "300px")
```

```{r checking_duplicates, include=FALSE}
dup <- myseries |>
  subset(duplicated (myseries))
#no duplicates
```

### Overall mean

```{r mean}
myseries |>
  features(Turnover, list(mean = mean)) |>
  arrange(mean)
```

### Quantile distribution

Here the data has been divided into 4 equal section, each containing 25% of the data.

```{r}
myseries |>
  features(Turnover, quantile)
```


## Statistical plots for data 

Figure \@ref(fig:fig1) shows that both the ends of this Q-Q plot deviates, and hence, has a fat tail but its center also does not follow a typically straight line. This means that the plot has many extreme values and the values vary throughout as well.

```{r fig1, fig.cap="Quantile-Quantile Plot"}
ggplot(myseries, aes(sample = Turnover)) +
  geom_qq() +
  geom_qq_line(color = "#6C03A0") +
  ylab("Sample Turnover Values ($Million AUD)") +
  xlab("Theoretical Values") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

Figure \@ref(fig:fig2) is a histogram and is somewhat similar to Bi-modal data, i.e., it represents 2 peaks. Here, it seems that both peaks have similar densities and the curve is sinusoidal. 

```{r fig2, fig.cap="Distribution of Turnover"}
ggplot(myseries, aes(x = Turnover)) +
  geom_histogram(aes(y = after_stat(density)), fill = "#6EBA6B") +
  geom_density(color = "#6C03A0") +
  ylab("Density") +
  xlab("Turnover ($Million AUD)") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```


### Exploring Time-Series Features

Figure \@ref(fig:fig3) shows that:

- Trend: There is somewhat an increasing trend visible in the data set.

- Seasonality: There seems to be multiplicative seasonality as there is a change in height of the values with time. There are strong peaks observed at the end of each year.

```{r fig3, fig.cap="Time Series"}
myseries |>
  autoplot(Turnover, color = "#C63F2C") +
  xlab("Year/Month") +
  ylab("Turnover ($Million AUD)") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

According to figure \@ref(fig:fig4),the highest turnovers are observed in December in each year and February has vales towards the lower end. This is possible due to the fact that hardware, building and garden work are outdoor tasks and is preferred to be done during summers and summer rises in December in Australia and starts to end by February.

```{r fig4, fig.cap="Seasonal Plot"}
myseries |>
  gg_season(Turnover,
            labels = "both") +
  ylab("Turnover ($Million AUD)") +
  scale_colour_viridis_c() +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

In figure \@ref(fig:fig5) the blue line shows the mean value for each month for all years. This plot confirms our observation from figure \@ref(fig:fig4) that the highest turnover was observed in December and lowest in February.

```{r fig5, fig.cap="Subseries Plot"}
myseries |>
  gg_subseries(Turnover, color = "#156A06") +
  ylab("Turnover ($Million AUD)") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
  
```

Below it is seen that a high value is present for the seasonal features.

```{r stl}
myseries |>
  features(Turnover, feat_stl)
```

Here in figure \@ref(fig:fig6), the colours indicate different months on the vertical axis. Most of the lag plots have shown positive correlation and hence, confirming strong seasonality in the data.

```{r fig6, fig.cap="Lag plot"}
myseries |>
  gg_lag(Turnover,
         lags = 1:24,
         geom = 'point',
         size = 0.5) +
  facet_wrap( ~ .lag, ncol = 6) +
  ylab("Turnover ($Million AUD)") +
  xlab("Lag") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

Autocorrelation is shown in figure \@ref(fig:fig7), i.e. a linear relationship between lagged values of time series. It shows that:

- There are peaks observed at every 12th lag, i.e. at the end of each month and hence, there is seasonality.This is also suggested by the fact that an equivalent amount of dip is observed (from the previous lag) at equal intervals.

- The *Scalloped" shape observed is also due to seasonality.

- This data has a trend as autocorrelations for the small lags are large and positive (as observations close in time are also close in value) and these positive values slowing decreases as lags increase.

```{r fig7, fig.cap="ACF"}
myseries |>
  ACF(Turnover, lag_max = 48) |>
  autoplot() +
  ylab("ACF") +
  xlab("Lag [1 Month]") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

Below is a summary of seven autocorrelation features (in this order):
first autocorrelation feature of the original data, sum of squares of first 10 coefficients from original data, first autocorrelation feature from differenced data, sum of squares of first 10 coefficients from differenced data, first autocorrelation feature from twice differenced data, sum of squares of first 10 coefficients from twice differenced data and autocorrelation coefficient at first seasonal lag.

```{r acf}
myseries |>
  features(Turnover, feat_acf)
```


# Transformations and differencing used (including unit-root test).

Transformations and difference is done to make the data stationary. There are 3 steps for this process:

- Check the changing variance in the data, if visible, **transform** the data with an appropriate lambda value. *Evaluate if the data is stationary and if no, move to the next step.*

- **Seasonal difference** to be carried out to remove seasonality, if the data is seasonal. This step also removes trend sometimes. *Evaluate if the data is stationary and if no, move to the next step.*

- Perform **regular difference** to remove any trend and anything else leftover.

## Step 1: Transformation

Figure \@ref(fig:fig8) shows a time series by transforming turnover to log(turnover) and it seems that there is still some trend and seasonality in the data, so this is not an appropriate transformation.

```{r fig8, fig.cap="Log Transformation"}
myseries |>
  autoplot(log(Turnover)) +
  ylab("Log Turnover") +
  xlab("Year Month") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

Therefore, performing the Guerrero test ahead to find out lambda value for box-cox transformation.

```{r lambda}
lambda <- myseries |>
  features(Turnover, features = guerrero)
lambda <- pull(lambda)

lambda
```

In figure \@ref(fig:fig9) box-cox transformation is performed on turnover with lambda= 0.4662629 and it still seems to have trend and seasonal pattern. The data is not stationary.

```{r fig9, fig.cap="Box-Cox Transformation"}
myseries |>
  autoplot(box_cox(Turnover, lambda)) +
  ylab("Transformed Turnover (lambda=0.467)") +
  xlab("Year Month") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

Figure \@ref(fig:fig10) shows the time plot and ACF together and adds a new plot to the report, i.e. PACF (will be used in section 3). 

```{r fig10, fig.cap="Timeplot, ACF, PACF with Transform"}
myseries |>
  gg_tsdisplay(box_cox(
    Turnover,
    lambda),plot_type = "partial")
```

There are still features of some trend and seasonality visible in the ACF, i.e. data is not stationary and therefore, requires seasonal difference.

### Step 2:  Seasonal Difference

Confirming conclusion from figure \@ref(fig:fig10) using unit root KPSS test which defines Null Hypothesis Ho= Data is Stationary.

```{r kpss}
myseries |>
  features(box_cox(
    Turnover, 
    lambda), unitroot_kpss)
```

Here the p value is 0.01, which actually means that the value is < 0.01 (due to a limit applied)
If p < 0.05, we need to difference the data. Hence, Ho is rejected.

Confirming if a seasonal difference is needed.

```{r nsdiff}
myseries |> 
  features(box_cox(
    Turnover, 
    lambda), unitroot_nsdiffs)
```

It is shown that 1 seasonal difference is needed.

In figure \@ref(fig:fig10), ACF shows that, the there is seasonality and is a monthly lag data, and hence, the lag is used as 12 for the first seasonal difference.

```{r fig11, fig.cap="After Transforming and Seasonal Differencing"}
myseries |> 
  gg_tsdisplay(difference(box_cox(
    Turnover, lambda), 
    lag=12), plot_type = "partial")
```

From \@ref(fig:fig11) can be said stationary as it is observed that:

- In the ACF, the data falls to 0 quickly after the 11th lag.

- The time plot shows unrelated values, i.e. not varying with time.

- There seems to be no seasonality or trend left in the data.

This is confirmed by unit root KPSS tests below:

```{r kpss_diff}
myseries |> 
  features(difference(box_cox(
    Turnover, lambda), lag=12), 
    unitroot_kpss)
```

Here the p value is 0.1 (or > 0.1) and can be said stationary.

Further checking if more difference is required:

```{r ndiff}
myseries |> 
  features(difference(box_cox(
    Turnover, lambda), lag=12), 
    unitroot_ndiffs)
```

This shows that no more difference is needed, i.e. no need to perform step 3 of evaluating a regular difference. Hence, the data is now stationary.

# Methodology used to create a short-list of appropriate ARIMA models and ETS models. (Includes AIC values as well as results from applying the models to a test-set consisting of the last 24 months of the data provided).

## ETS

For this a test and a train data set is prepared with test set being for the last 24 years, i.e. 2017 and 2018 and rest being the training set.

```{r test_train}
test <- myseries |>
  filter(Month >= yearmonth("2017 Jan"))

train <- myseries |>
  filter(Month < yearmonth("2017 Jan"))
```

### Shortlist of ETS models with results

Observing figure \@ref(fig:fig9), we can say that error and seasonality are can be additive or multiplicative. There is somewhat an upward trend which can be called as additive. The appropriate models could be (MAM), (MNM) or (MAdM). Also here the seasonality component we can also use the additive component to the seasonality which leads to ETS models (M,N,A) or (A,Ad,A)

```{r etsfit}
ets_fit <- train |>
  model(
    MAM = ETS(box_cox(Turnover, lambda) ~ error("M") + trend("A") + season("M")),
    MNM = ETS(box_cox(Turnover, lambda) ~ error("M") + trend("N") + season("M")),
    MAdM = ETS(
      box_cox(Turnover, lambda) ~ error("M") + trend("Ad") + season("M")
    ),
    MNA = ETS(box_cox(Turnover, lambda) ~ error("M") + trend("N") + season("A")),
    MAdM = ETS(
      box_cox(Turnover, lambda) ~ error("A") + trend("Ad") + season("A")
    ),
    ets_auto = ETS(box_cox(Turnover, lambda))
  )

glance(ets_fit)
```

Here the results are discussed for the shortlisted 3 models and the automatic model chosen. It is observed that the AICc value is the lowest for the automatic ETS model created.

```{r reportetsbest}
ets_fit |>
  select(ets_auto) |>
  report()
```

The best model reported here is (ANA)

```{r accuracy}
ets_fit |>
  forecast(h = 24) |>
  accuracy(myseries)
```

We observe that the models MAdM and ANA have the same results other than just one where the value for ANA is the lowest and hence we select ETS(ANA) as the best model.

## ARIMA

Referring to figure \@ref(fig:fig10), we find the (pdq) and (PDQ)12 (lag 12). We know that d=0, D=1 (seasonal difference)

For q we look at ACF plot and significant lags before 12 is only 1st, can be 2nd too. So q=1 or can be q=2. In PACF, significant lag is on 13th , so p=1 .

Hence (p,d,q)= (1,0,1) (2,0,1) (0,0,1) (2,0,0) (1,0,2) (2,0,2) (0,0,2) (2,0,0)
Also, p=0 (when we only observe ACF) and q=0 (when we only observe PACF)

For seasonal values, we only look at seasonal lags, i.e. multiples of 12.
Looking at ACF, Q=0 as ACF quickly decays to 0 and the first lag can be significant so Q=1. And from PACF, only lag at 12  and 24 seems to have  significant lags, so P=2. Also even the first lag is not as significant so P=0

Hence (P,D,Q)= (2,1,0) (0,1,1)

Looking at the time plot, the average is not the center so a constant can be present.

### Shortlisting of ARIMA models with results

```{r fit_arima}
arima_fit <- train |> 
  model(arima101210=ARIMA(box_cox(Turnover, lambda)~pdq(1,0,1)+ PDQ(2,1,0)),
        arima201210=ARIMA(box_cox(Turnover, lambda)~pdq(2,0,1)+ PDQ(2,1,0)),
        arima001210=ARIMA(box_cox(Turnover, lambda)~pdq(0,0,1)+ PDQ(2,1,0)),
        arima201210=ARIMA(box_cox(Turnover, lambda)~pdq(2,0,1)+ PDQ(2,1,0)),
        arima101011=ARIMA(box_cox(Turnover, lambda)~pdq(1,0,1)+ PDQ(0,1,1)),
        arima102111=ARIMA(box_cox(Turnover, lambda)~pdq(1,0,2)+ PDQ(1,1,1)),
        arima_auto=ARIMA(box_cox(Turnover, lambda), trace=TRUE))
        
  
tidy(arima_fit)
```


Here the estimate parameter shows that it has some constants.

```{r glance}
glance(arima_fit)
```

This shows the lowest AICc value is for the automatic arima model which is just slightly lower than (102)(111) model

```{r report_arima}
arima_fit |>
  select(arima_auto) |>
  report()
```

The automatic ARIMA reported is (102)(011) which is the best model.

```{r accuracy_arima}
arima_fit |>
  forecast(h = 24) |>
  accuracy(myseries)
```

# Choosing one ARIMA model and one ETS model based on this analysis and showing parameter estimates, residual diagnostics, forecasts and prediction intervals for both models. Also diagnostic checking for both models including ACF graphs and the Ljung-Box test is shown. 

The best ETS model chosen is ANA and the best ARIMA model is (102)(011)

## Parameter estimates

```{r tidyets}
ets_fit |>
  select(ets_auto) |>
  tidy()
```


```{r tidyarima}
arima_fit |>
  select(arima_auto) |>
  tidy()
```

## ETS

### Residuals

Figure \@ref(fig:fig12) shows residuals for best ETS model. It is observed that it is fairly normally distributed. ACF shows somewhat white noise, i.e. there are some residuals left but they are not significant.

```{r fig12, fig.cap="Residuals ETS"}
gg_tsresiduals(ets_fit |>
                 select(ets_auto))
```

### Forecast

```{r fig13, fig.cap="Forecast for ETS best model ANA"}
ets_fit  |>
  select(State, Industry, ets_auto) |>
  forecast(h = 24) |>
  autoplot(myseries) +
  ylab("Turnover ($Million AUD)") +
  xlab("Year Month") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```


```{r augement_ets}
ets_fit |>
  select(ets_auto) |>
  augment()
```

### Prediction Intervals

```{r pred_ets}
ets_fit |>
  select(ets_auto) |>
  forecast(h = 24) |>
  hilo(level = 95)
```

### ACF for ETS best model

Zooming in on figure \@ref(fig:fig12), figure \@ref(fig:fig14) suggests autocorrelation.

```{r fig14, fig.cap="ACF Diagnostics for ETS best model"}
augment(ets_fit) |>
  filter(.model == "ets_auto") |>
  ACF(.innov) |>
  autoplot() +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

###  Ljung-Box test

This test uses the Hypothesis Ho that the residuals are independently distributed.

```{r lj_ets}
augment(ets_fit) |>
  filter(.model == "ets_auto") |>
  features(.innov, ljung_box, lag = 24, dof = 18)
```

Here the p value is <0.05, suggesting that the values are auto correlated, rejecting the null hypothesis.

## ARIMA

Figure \@ref(fig:fig15) shows a normal distribution and white noise.

```{r fig15, fig.cap="Residuals ARIMA"}
gg_tsresiduals(arima_fit |>
                 select(arima_auto))
```

### Forecast

```{r fig16, fig.cap=" Forecast for best ARIMA model (102)(011)"}
arima_fit  |>
  select(State, Industry, arima_auto) |>
  forecast(h = 24) |>
  autoplot(myseries) +
  ylab("Turnover ($Million AUD)") +
  xlab("Year Month") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

```{r augment_arima}
arima_fit |>
  select(arima_auto) |>
  augment()
```

### Prediction Intervals

```{r pred_in_arima}
arima_fit |>
  select(arima_auto) |>
  forecast(h = 24) |>
  hilo(level = 95)
```

### Diagnostic test: ACF

Figure \@ref(fig:fig17) shows that most residuals are within the blue line, showing white noise.

```{r fig17, fig.cap="ACF for best fit ARIMA model"}
augment(arima_fit) |>
  filter(.model == "arima_auto") |>
  ACF(.innov) |>
  autoplot() +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

###  Ljung-Box test

```{r Lj_arima}
augment(arima_fit) |>
  filter(.model == "arima_auto") |>
  features(.innov, ljung_box, lag = 36, dof = 24)
```

Here, p value is <0.05 rejecting the Null Hypothesis.

# Comparison of the results from each of the preferred models and the best method for better forecasts (reference to the test-set).

## Comparison

- Both the models show somewhat a normal distribution and white noise in the data (figure \@ref(fig:fig12) and \@ref(fig:fig15).

- The forecasts are similar for both models, but when augmented, the ETS has more negative values in .resid and .innov.

- The prediction intervals in the ARIMA model are narrower than the ETS model.

- Both show a value <0.05 in the Ljung test.

## Best method for better forecast for test set

```{r fig18, fig.cap="Best model forecast for test set"}
both_best <- train |>
  model(ets_auto = ETS(box_cox(Turnover, lambda)),
        arima_auto = ARIMA(box_cox(Turnover, lambda)))

both_best |>
  forecast(h = 24) |>
  autoplot(test) +
  ylab("Turnover ($Million AUD)") +
  xlab("Year Month") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )

```

Comparing figure \@ref(fig:fig18) with \@ref(fig:fig13) which is the overall forecast and comparison between both best models too, it shows that prediction intervals are narrow in best ARIMA.

```{r acc_both}
both_best |>
  forecast(h = 24) |>
  accuracy(myseries)
```

The accuracy values are lower for best arima model, suggesting better forecasts.


# Applying the two chosen models to the full data set, re-estimating the parameters but not changing the model structure. Producing out-of-sample point forecasts and 80% prediction intervals for each model for two years past the end of the data provided.

## Dataset on full data

```{r best_fulldata}
both_best2 <- myseries |>
  model(ets_ANA = ETS(box_cox(Turnover, lambda)),
        arima_102011 = ARIMA(box_cox(Turnover, lambda)))
```

## Re-estimating parameters

### Estimates

```{r est_best}
both_best2 |>
  tidy()
```

### Residuals

```{r resid_best}
gg_tsresiduals(both_best2 |>
                 select(ets_ANA)) +
  ggtitle("ETS ANA Residuals on full dataset")

gg_tsresiduals(both_best2 |>
                 select(arima_102011)) +
  ggtitle("ARIMA (102)(011) Residuals on full dataset")
```

### Model diagnostics

```{r aug_best}
both_best2 |>
  select(ets_ANA) |>
  augment()

both_best2 |>
  select(arima_102011) |>
  augment()
```


### Prodocuing out of sample forecasts

Figure \@ref(fig:fig19) shows forecast of two best models on full data set with 80% prediction interval.

```{r fig19, fig.cap="80% interval forecast by best models"}
both_best2 |>
  forecast(h = 24) |>
  autoplot(myseries, level = 80) +
  ylab("Turnover ($Million AUD)") +
  xlab("Year Month") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

# Obtaining up-to-date data from the [ABS website](https://www.abs.gov.au/statistics/industry/retail-and-wholesale-trade/retail-trade-australia). - using the previous release of data, rather than the latest release and comparing above forecasts with the actual numbers.

```{r absdata}
abs_retail_data <- read_abs("8501.0", tables="11")

```

```{r abs}
abs <- abs_retail_data |>
  separate(series,
           into = c("extra", "State", "Industry"),
           sep = ";") |>
  mutate(
    Month = yearmonth(date),
    State = trimws(State),
    Industry = trimws(Industry)
  ) |>
  select(Month, State, value, Industry) |>
  filter(State == "South Australia") |>
  filter(Industry == "Hardware, building and garden supplies retailing") |>
  rename(Turnover = value) |>
  filter(Month > yearmonth("2018 Dec"))
```

Comparing figures \@ref(fig:fig20) and \@ref(fig:fig21), we see that ETS model has given lagged values whereas arima model values are closer to the actual values.

```{r fig20, fig.cap="ABS data plot"}
abs |>
  ggplot(aes(x = Month, y = Turnover)) +
  geom_line() +
  ylab("Turnover ($Million AUD)") +
  xlab("Year Month") +
  theme_minimal() +
  theme(
    plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(
      color = "#993333",
      angle = 45,
      size = 8
    ),
    axis.text.y = element_text(colour = "#08007F", size = 8)
  )
```

```{r fig21, fig.cap="Forecast from myseries data"}
both_best2 |> 
  forecast(h=51) |> 
  autoplot()+
    ylab("Turnover ($Million AUD)")+
  xlab("Year Month")+
  theme_minimal()+
   theme( plot.background = element_rect(fill = "#FFF8F7"),
    panel.grid.major.y = element_blank(),
        axis.text.x = element_text(color="#993333", angle=45, size=8),
    axis.text.y = element_text(colour= "#08007F", size=8))
```

The ABS data contain 51 more months than the myseries data. And hence, forecasting is performed for 51 months in figure \@ref(fig:fig21).

# Benefits and limitations of the models for the data.

- The ETS model chosen was ANA and ARIMS model chosen was (102)(011).

- ETS models are considered to be stationary and ARIMA models are considered non-stationary. 

- ARIMA models fits the training data slightly better than ETS.

- ETS modeling gives more significance to recent observation.

- ETS was unpredictable as it predicted ANA and could not handle trend well.

- The value of ETS forecasts when compared to the actual values, were lagging behind as compared to ARIMA forecasts.

- Outliers affect ARIMA modeling results and hence a duplicate test was done before the modeling process.

Below the accuracy for both models are compared with reference to the test set and it is see that ARIMA has better values and therefore, slightly more accurate.

```{r accuracy_all}
bind_rows(
  arima_fit |> select(arima_auto) |> accuracy(),
  ets_fit |>  select(ets_auto) |> accuracy(),
  arima_fit |> select(arima_auto) |> forecast(h = 10) |> accuracy(test),
  ets_fit |>  select(ets_auto) |> forecast(h = 10) |>  accuracy(test)
)
```


# References 

Australian retail trade turnover. rdrr.io. https://rdrr.io/cran/tsibbledata/man/aus_retail.html

Chang, W. (2023, May 16). 13.13 Creating a QQ Plot | R Graphics Cookbook, 2nd edition. https://r-graphics.org/recipe-miscgraph-qq

Coder, R. (2021). Histogram with density in ggplot2. R CHARTS | a Collection of Charts and Graphs Made With the R Programming Language. https://r-charts.com/distribution/histogram-density-ggplot2/

Forecasting: Principles and Practice (3rd ed). (n.d.). https://otexts.com/fpp3/

Holtz, Y. (n.d.). The R Graph Gallery – Help and inspiration for R charts. The R Graph Gallery. https://r-graph-gallery.com/

Retail Trade, Australia, March 2023. (2023, May 9). Australian Bureau of Statistics. https://www.abs.gov.au/statistics/industry/retail-and-wholesale-trade/retail-trade-australia/latest-release

Robjhyndman. (n.d.). fpp3package/README.Rmd at master · robjhyndman/fpp3package. GitHub. https://github.com/robjhyndman/fpp3package/blob/master/README.Rmd

Roy, A. (2023). What is Sales Forecasting? CX Today. https://www.cxtoday.com/contact-centre/what-is-sales-forecasting/